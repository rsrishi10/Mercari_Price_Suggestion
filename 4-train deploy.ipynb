{"cells":[{"cell_type":"code","execution_count":65,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3703,"status":"ok","timestamp":1687706828981,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"},"user_tz":-330},"id":"mPKnmrJgqq32","outputId":"9cf0b980-83ab-4bdc-9d36-2fae2836cb44"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":66,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1687706828981,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"},"user_tz":-330},"id":"0QJOq9G5qrCx","outputId":"6f6f40a3-d811-47fa-8a0a-5cc958aeb094"},"outputs":[{"name":"stdout","output_type":"stream","text":["total 1713858\n","-rw------- 1 root root    2441360 Jun 23 10:05  1-EDA.ipynb\n","-rw------- 1 root root      40868 Jun 25 11:18 '2-featureization and pickleing.ipynb'\n","-rw------- 1 root root      46260 Jun 25 15:25  3_Hyperparameter_tuning.ipynb\n","-rw------- 1 root root      44151 Jun 25 15:26 '4-train deploy.ipynb'\n","-rw------- 1 root root     122880 Jun 24 19:02  Mercari_to1.db\n","drwx------ 2 root root       4096 Jun 23 11:42  pickle\n","-rw------- 1 root root   11853920 Jun 25 11:18  price_log.pickle\n","-rw------- 1 root root 1402624694 Jun 25 11:18  tfidf.pickle\n","-rw------- 1 root root  337809843 Nov 11  2017  train.tsv\n"]}],"source":["import os\n","os.chdir(\"/content/drive/My Drive/mercari_capstone2\")\n","!ls -l"]},{"cell_type":"code","execution_count":67,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":311},"executionInfo":{"elapsed":9736,"status":"ok","timestamp":1687706838715,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"},"user_tz":-330},"id":"d1_aogfEH6a0","outputId":"b81ce279-b5ac-4646-9774-beda7a456e6e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found existing installation: scikit-learn 1.0.2\n","Uninstalling scikit-learn-1.0.2:\n","  Successfully uninstalled scikit-learn-1.0.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scikit-learn==1.0.2\n","  Using cached scikit_learn-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n","Requirement already satisfied: numpy\u003e=1.14.6 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.22.4)\n","Requirement already satisfied: scipy\u003e=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.10.1)\n","Requirement already satisfied: joblib\u003e=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.2.0)\n","Requirement already satisfied: threadpoolctl\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (3.1.0)\n","Installing collected packages: scikit-learn\n","Successfully installed scikit-learn-1.0.2\n"]},{"data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["sklearn"]}}},"metadata":{},"output_type":"display_data"}],"source":["!pip uninstall scikit-learn -y\n","!pip install scikit-learn==1.0.2"]},{"cell_type":"code","execution_count":68,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1687706838716,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"},"user_tz":-330},"id":"faDikRZQqrDj","outputId":"6e3cacac-ece1-4069-917a-9cc2b0c6e415"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["#importing modules/libraries\n","import pandas as pd\n","import numpy as np\n","import scipy\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import sys\n","import os\n","import psutil\n","# from scipy.stats import randint as sp_randint\n","# from scipy.stats import uniform as sp_uniform\n","\n","\n","from tqdm.notebook import tqdm\n","# from collections import Counter\n","# from collections import defaultdict\n","import re as regex\n","import random\n","# from random import sample\n","# from bs4 import BeautifulSoup\n","import pickle\n","import inspect\n","import time\n","\n","import sklearn\n","from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, LabelBinarizer,OneHotEncoder\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import mean_squared_error\n","import lightgbm as lgb\n","from sklearn.linear_model import Lasso,Ridge\n","\n","\n","import string\n","# import emoji\n","# from wordcloud import WordCloud\n","import nltk\n","nltk.download(\"stopwords\")\n","# nltk.download(\"brown\")\n","# nltk.download(\"names\")\n","# nltk.download('punkt')\n","nltk.download('wordnet')\n","# nltk.download('averaged_perceptron_tagger')\n","# nltk.download('universal_tagset')\n","# from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem.wordnet import WordNetLemmatizer\n","# from nltk.stem.porter import PorterStemmer\n","\n","\n","\n","\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":69,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1687706838716,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"},"user_tz":-330},"id":"ToV2mTtyj0mq"},"outputs":[],"source":["def Datapoint():\n","\n","  data_dict = {}\n","\n","  name = input('Your product name\\n')\n","\n","\n","\n","  item_condition_id  = int(input('Condition of your product ranging from 1 to 5 from good to worse \\n'))\n","\n","\n","  category_name  = input('What all Categories your product belongs to separeted by a \"/\" \\n')\n","\n","\n","  brand_name  = input('What brand is your product \\n')\n","\n","\n","  shipping\t  = int(input('0 for shipping charges/ 1 for no shipping charges\\n'))\n","\n","\n","  item_description  = input('describe your product succintly \\n')\n","\n","  if name == '':\n","    name ='abs'\n","  if brand_name == '':\n","    brand_name ='abs'\n","  if item_description == '':\n","    item_description ='No Description Yet'\n","\n","  data_dict['name'] = name.strip()\n","  data_dict['item_condition_id'] = item_condition_id\n","  data_dict['category_name'] = category_name.strip()\n","  data_dict['brand_name'] = brand_name.strip()\n","  data_dict['shipping'] = shipping\n","  data_dict['item_description'] = item_description.strip()\n","\n","  df = pd.DataFrame.from_records([data_dict])\n","\n","  df['item_condition_id'] = df['item_condition_id'].astype('category')\n","  df['shipping'] = df['shipping'].astype('category')\n","  df['name'] = df['name'].astype('object')\n","  df['category_name'] = df['category_name'].astype('object')\n","  df['brand_name'] = df['brand_name'].astype('object')\n","  df['item_description'] = df['item_description'].astype('object')\n","\n","  return df"]},{"cell_type":"code","execution_count":70,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1687706838716,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"},"user_tz":-330},"id":"MtTie9hYqrG3"},"outputs":[],"source":["def Impute(df,clock):\n","  # imputing with a 'abs' or absent\n","    # df['category_name'].fillna(value='abs', inplace=True)\n","    df['name'].fillna(value='abs', inplace=True)\n","    df['item_description'].fillna(value='No Description Yet', inplace=True)\n","\n","    # using brands from train data only\n","\n","\n","  # imputing with a 'abs' or absent for train data and using  brand\n","  # names form train data only as target and freqency encoding done\n","  # in later sections ofnotebook so avoiding data leakage\n","    with open('pickle/impute_brand_name','rb') as f:\n","      brand_name = pickle.load(f)\n","    df.loc[~df['brand_name'].isin(brand_name),'brand_name'] = 'abs'\n","\n","    with open('pickle/impute_category_name','rb') as f:\n","      category_name = pickle.load(f)\n","    df.loc[~df['category_name'].isin(category_name),'category_name'] = 'abs'\n","\n","   # print fucntion completion time and with function name\n","    print(f'[{round((time.time() - clock),2)}] {inspect.stack()[0][3]} completed')\n","    del brand_name,category_name\n","    return df"]},{"cell_type":"code","execution_count":71,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1687706838716,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"},"user_tz":-330},"id":"YojJ7_nw2els"},"outputs":[],"source":["# a category column contains Nan or 3 or more sub category in it upto 5\n","# as rows with more than 3 or less than 3 categories are less than 0.1 percent,\n","# we make only 3 new cols with segregated category names\n","def sub_cat(row):\n","      try:\n","        split = row.split('/')\n","        if len(split) \u003e= 3:\n","          return split[0],split[1],split[2]\n","        if len(split) == 2:\n","          return split[0], split[1], 'abs'\n","        elif len(split) == 1:\n","          return split[0], 'abs', 'abs'\n","        else:\n","          return 'abs', 'abs', 'abs'\n","      except Exception:\n","          return  'abs', 'abs', 'abs'\n","\n","# extracting extra features from data\n","def Extract_features(df,clock):\n","  # regex used later in this section to count number of them in a text column\n","    RE_PUNCTUATION = '|'.join([regex.escape(x) for x in string.punctuation])\n","    non_alphanumpunct = regex.compile(u'[^A-Za-z0-9\\.?!,; \\(\\)\\[\\]\\'\\\"\\$]+')\n","\n","  # extracting sub categories\n","    print(f'[{round(time.time()-clock)}]Extracting Subcat')\n","    df['sc1'], df['sc2'],df['sc3'] =  zip(*df['category_name'].apply(sub_cat))\n","    df.drop(columns='category_name',inplace = True)\n","    df['sc1'] = df['sc1'].astype('category')\n","    df['sc2'] = df['sc2'].astype('category')\n","    df['sc3'] = df['sc3'].astype('category')\n","\n","  # has description or not/ missing value added as a feature\n","    print(f'[{round(time.time()-clock)}]Extracting HasDescription ')\n","    df['HasDescription'] = 1\n","    df.loc[df['item_description']=='No description yet', 'HasDescription'] = 0\n","    df['HasDescription'] =df['HasDescription'].astype('category')\n","\n","  # has price or not/ [rm] values in textual columns are indicative of presence\n","  # price in the data which has been cleaned as suggested by the compition itself\n","    print(f'[{round(time.time()-clock)}]Extracting HasPrice ')\n","    df['HasPrice'] = 0\n","    df.loc[df['item_description'].str.contains('[rm]', regex=False), 'HasPrice'] = 1\n","    df.loc[df['name'].str.contains('[rm]', regex=False), 'HasPrice'] = 1\n","    df['HasPrice'] =df['HasPrice'].astype('category')\n","\n","  # counting number of tokens in textual columns\n","    print(f'[{round(time.time()-clock)}]Extracting Token Count ')\n","    df['NameTokenCount'] = df['name'].str.split().apply(len)\n","    df['DescTokenCount'] = df['item_description'].str.split().apply(len)\n","    df['NameTokenCount'] = df['NameTokenCount'].astype('uint32')\n","    df['DescTokenCount'] = df['DescTokenCount'].astype('uint32')\n","\n","  # ratio of token token counts in name and description columns(2 textual cols)\n","\n","    print(f'[{round(time.time()-clock)}]Extracting Name to Desc token Ratio ')\n","    df['NameDescTokenRatio'] = df['NameTokenCount']/df['DescTokenCount']\n","    df['NameDescTokenRatio'] =df['NameDescTokenRatio'].astype('float32')\n","\n","  # adding missing value as a feature for brand\n","    print(f'[{round(time.time()-clock)}]Extracting HasBrand ')\n","    df['HasBrand'] =1\n","    df.loc[df['brand_name'] == 'abs', 'HasBrand'] = 0\n","    df['HasBrand'] =df['HasBrand'].astype('category')\n","\n","  # counting uppper and lower count of characters as EDA suggested phoney/\n","  # counterfiet items when listed uses too many bold and Caps charactes with emojis\n","    print(f'[{round(time.time()-clock)}]Extracting Lower count ')\n","    df['NameLowerCount'] = df.name.str.count('[a-z]')\n","    df['DescriptionLowerCount'] = df.item_description.str.count('[a-z]')\n","    df['NameLowerCount'] =df['NameLowerCount'].astype('uint32')\n","    df['DescriptionLowerCount'] =df['DescriptionLowerCount'].astype('uint32')\n","\n","\n","    print(f'[{round(time.time()-clock)}]Extracting Upper count ')\n","    df['NameUpperCount'] = df.name.str.count('[A-Z]')\n","    df['DescriptionUpperCount'] = df.item_description.str.count('[A-Z]')\n","    df['NameUpperCount'] =df['NameUpperCount'].astype('uint32')\n","    df['DescriptionUpperCount'] =df['DescriptionUpperCount'].astype('uint32')\n","\n","  # punctuation count\n","    print(f'[{round(time.time()-clock)}]Extracting Punctuation Count ')\n","    df['NamePunctCount'] = df.name.str.count(RE_PUNCTUATION)\n","    df['DescriptionPunctCount'] = df.item_description.str.count(RE_PUNCTUATION)\n","    df['NamePunctCount'] = df['NamePunctCount'].astype('uint32')\n","    df['DescriptionPunctCount'] = df['DescriptionPunctCount'].astype('uint32')\n","\n","  # punct count ratio\n","    print(f'[{round(time.time()-clock)}]Extracting Punctuation Ratio ')\n","    df['NamePunctCountRatio'] = df['NamePunctCount'] / df['NameTokenCount']\n","    df['DescriptionPunctCountRatio'] = df['DescriptionPunctCount'] / df['DescTokenCount']\n","    df['NamePunctCountRatio'] =df['NamePunctCountRatio'].astype('float32')\n","    df['DescriptionPunctCountRatio'] =df['DescriptionPunctCountRatio'].astype('float32')\n","\n","  # digit count( if model can get a sense of bundled items)\n","    print(f'[{round(time.time()-clock)}]Extracting Digit count ')\n","    df['NameDigitCount'] = df.name.str.count('[0-9]')\n","    df['DescriptionDigitCount'] = df.item_description.str.count('[0-9]')\n","    df['NameDigitCount'] =df['NameDigitCount'].astype('uint32')\n","    df['DescriptionDigitCount'] =df['DescriptionDigitCount'].astype('uint32')\n","\n","  # emoji and/or other nonalphanum count\n","    print(f'[{round(time.time()-clock)}]Extracting NonAlphaNum count ')\n","    df['NonAlphaDescCount'] = df['item_description'].str.count(non_alphanumpunct)\n","    df['NonAlphaNameCount'] = df['name'].str.count(non_alphanumpunct)\n","    df['NonAlphaDescCount'] = df['NonAlphaDescCount'].astype('uint32')\n","    df['NonAlphaNameCount'] = df['NonAlphaNameCount'].astype('uint32')\n","\n","\n","    cols = set(df.columns.values)\n","    non_num_col = {'name', 'item_condition_id', 'brand_name',\n","                  'shipping', 'item_description', 'sc1',\n","                  'sc2', 'sc3','HasDescription','HasPrice','HasBrand'\n","                  }\n","\n","    cols_to_normalize = cols - non_num_col\n","\n","  # normalizing all the counts and ratios\n","    print(f'[{round(time.time()-clock)}]Normalizing')\n","    df_to_normalize = df[list(cols_to_normalize)]\n","    with open('pickle/normalize_min','rb') as f:\n","      min = pickle.load(f)\n","    with open('pickle/normalize_max','rb') as f:\n","      max = pickle.load(f)\n","    df_to_normalize = (df_to_normalize - min) / (max - min)\n","\n","\n","    df = df[list(non_num_col)]\n","    df = pd.concat([df, df_to_normalize],axis=1)\n","\n","    # df.drop(columns='index',inplace=True)\n","\n","    del(df_to_normalize)\n","\n","\n","    '''  extracting mean categorical and brand price with minding data leakage with addition of\n","    random noise so making data more robust. An idea taken form a some youtube video of a\n","    kaggle grandmaster . This noise addition clearly has impacted the performace of model very positively. '''\n","\n","    print(f'[{round(time.time()-clock)}]Extracting Mean price Categories')\n","    mean_dc = {}\n","\n","    lst = ['sc1','sc2','sc3', 'brand_name']\n","\n","  #imputing values for nan with mean\n","    def boundary_case(hmap,key):\n","      try:\n","        return float(hmap[key])*np.random.normal(1,0.1)\n","      except:\n","        '''  when cases in test data are not\n","        present in train data mean_dict[feat] returns a nan to tackle that this part\n","        has been added( tho with normal usage it does not occur as this has been\n","        taken care of in  the imputation part itself , i had an experiment run which\n","        produced those cases so made this part as permanent only) '''\n","        np.mean(list(hmap.values()))*np.random.normal(1,0.1)\n","\n","\n","    for feat in lst:\n","        ''' for every categorical column in the list above  finding the mean price of\n","        every category in it and adding that price in a column with a noise added to it\n","        *np.randon.normal(1,0.1)'''\n","        with open(f'pickle/mean_dc_{feat}','rb') as f:\n","          hmap = pickle.load(f)\n","\n","        df['MeanPrice_'+feat] = df[feat].apply(lambda x : boundary_case(hmap,str(x))).astype(np.float32)\n","        df['MeanPrice_'+feat].fillna(np.mean(list(hmap.values()))*np.random.normal(1,0.1), inplace=True  )\n","\n","\n","\n","    print(f'[{round((time.time() - clock),2)}] {inspect.stack()[0][3]} completed')\n","\n","    return df"]},{"cell_type":"code","execution_count":72,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1687706838717,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"},"user_tz":-330},"id":"ISKXX4lOqrKK"},"outputs":[],"source":["def Make_text_column(df,clock):\n","    '''As we saw in EDA that brands with NAN values can be imputed with names and item_desciption\n","    columns as there are brand names prevelent with more than 40 to 45 percent of chance.\n","    So instead of imputing so many brands (43 percent), just creating a new column by merging\n","    brands_name, name and item_description and making a text column and letting tfidf taking care of it.\n","    '''\n","\n","    df['text'] = df['name'].astype(str)+' '+df['brand_name'].str.strip().astype(str)+' '+df['item_description'].str.strip().astype(str)\n","    df = df.drop(columns=['item_description'])\n","\n","\n","    def decontracted(text):\n","    # tried many kinds of regex to clean the data but final result wasnt effected much wiht\n","    # this part so only doing necessary onces\n","        try:\n","            text = regex.sub(u\"won't\", \"will not\", text)\n","            text = regex.sub(u\"can\\'t\", \"can not\", text)\n","            text = regex.sub(u\"n\\'t\", \" not\", text)\n","            text = regex.sub(u\"\\'t\", \" not\", text)\n","            # separating digits for a sense of count if bundled items sold\n","            text = u\" \".join(regex.split('(\\d+)',text) )\n","\n","        except:\n","            print('error')\n","        return text\n","\n","    def clean(df,col):\n","        non_alphanums = regex.compile(u'[^A-Za-z0-9 ]+')\n","        # wl = WordNetLemmatizer()\n","\n","        preprocessed_text = []\n","        for _,sentance in tqdm(df[col].iteritems(),total=df.shape[0]):\n","\n","            sentance = decontracted(sentance)\n","\n","            # nonalphanumeric character removal\n","            sentance = non_alphanums.sub(u' ', sentance)\n","\n","            ''' did not lemmatize cause takes a lot of time and has negligible to no\n","            effect on performance\n","            did not convert to lower case as this takes a lot of time and can be done\n","            interensicly within TFIDF and Count vectorization along with text standardization'''\n","            # lemmetizing\n","            # sentance = ' '.join(wl.lemmatize(word.strip()) for word in sentance.split())\n","            sentance = ' '.join(word.strip() for word in sentance.split())\n","\n","            preprocessed_text.append(sentance)\n","        df[col] = pd.Series(preprocessed_text).values\n","        del preprocessed_text\n","        return df\n","\n","\n","    print('Cleaning text')\n","    df= clean(df,'text')\n","    print(f'Done')\n","\n","    print(f'[{round((time.time() - clock),2)}] {inspect.stack()[0][3]} completed')\n","\n","\n","    return df"]},{"cell_type":"code","execution_count":73,"metadata":{"executionInfo":{"elapsed":706,"status":"ok","timestamp":1687706839416,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"},"user_tz":-330},"id":"_jckn-IjqrN1"},"outputs":[],"source":["def high_categorical(df,col='brand_name'):\n","        ''' As brand has close to 5000 categories, converting it to numbers\n","        by Frequency encoding brand with minding data leakage, Tho  lgbm handles high order\n","        categorical columns efficiently this encoding boosted performance instead of just\n","        using OHE, also it is with addition of random noise so making data more robust and\n","        enhancing performance I had chosen to do the same with sub categorical columns but the\n","        did not perform that well and took extra memory space too'''\n","        with open('pickle/brand_freq_dict','rb') as f:\n","          dict_replace = pickle.load(f)\n","        col_cat: pd.Series.astype('float16') = df[col].map(dict_replace)\n","\n","        return col_cat.values.reshape(-1,1)\n","\n","'''Converting all the data till yet to numeric form if not yet done'''\n","def Convert_to_predictor(df,clock,stopwords=stopwords,high_categorical=high_categorical):\n","    try:\n","      df.drop(columns='index',inplace = True)\n","    except:\n","      pass\n","\n","    with open('pickle/item_condition_id','rb') as f:\n","      cv = pickle.load(f)\n","    item_condition_id = cv.transform(df['item_condition_id'].astype(int).iloc[:].values.reshape(-1,1))\n","    print(f'[{round((time.time() - clock),2)}]Transform item_condition_id  data completed.{item_condition_id.shape}')\n","\n","\n","    with open('pickle/shipping','rb') as f:\n","      cv = pickle.load(f)\n","    shipping = cv.transform(df['shipping'].astype(int).iloc[:].values.reshape(-1,1))\n","    print(f'[{round((time.time() - clock),2)}]Transform shipping  data completed.{shipping.shape}')\n","\n","\n","    with open('pickle/HasDescription','rb') as f:\n","      cv = pickle.load(f)\n","    HasDescription = cv.transform(df['HasDescription'].astype(int).iloc[:].values.reshape(-1,1))\n","    print(f'[{round((time.time() - clock),2)}]Transform HasDescription  data completed.{HasDescription.shape}')\n","\n","\n","    with open('pickle/HasPrice','rb') as f:\n","      cv = pickle.load(f)\n","    HasPrice = cv.transform(df['HasPrice'].astype(int).iloc[:].values.reshape(-1,1))\n","    print(f'[{round((time.time() - clock),2)}]Transform HasPrice  data completed.{HasPrice.shape}')\n","\n","\n","    with open('pickle/HasBrand','rb') as f:\n","      cv = pickle.load(f)\n","    HasBrand = cv.transform(df['HasBrand'].astype(int).iloc[:].values.reshape(-1,1))\n","\n","    print(f'[{round((time.time() - clock),2)}]Transform HasBrand  data completed.{HasBrand.shape}')\n","\n","    with open('pickle/sc1','rb') as f:\n","      cv = pickle.load(f)\n","    sc1 = cv.transform(df.sc1.astype(str).iloc[:].values.reshape(-1,1))\n","    print(f'[{round((time.time() - clock),2)}]Transform sc1  data completed.{sc1.shape}')\n","\n","    with open('pickle/sc2','rb') as f:\n","      cv = pickle.load(f)\n","    sc2 = cv.transform(df.sc2.astype(str).iloc[:].values.reshape(-1,1))\n","    print(f'[{round((time.time() - clock),2)}]Transform sc2  data completed.{sc2.shape}')\n","\n","    with open('pickle/sc3','rb') as f:\n","      cv = pickle.load(f)\n","    sc3 = cv.transform(df.sc3.astype(str).iloc[:].values.reshape(-1,1))\n","    print(f'[{round((time.time() - clock),2)}]Transform sc3  data completed.{sc3.shape}')\n","\n","\n","\n","\n","    df_dummies = scipy.sparse.hstack([item_condition_id, shipping, HasDescription, HasPrice,HasBrand,sc1,sc2,sc3])\n","    print(df_dummies.shape)\n","    df.drop(columns=['item_condition_id', 'shipping','HasDescription', 'HasPrice','HasBrand'],inplace=True)\n","    df.drop(columns=['sc1','sc2','sc3'],inplace=True)\n","\n","    print(f'[{round((time.time() - clock),2)}]Transform categories data completed.')\n","\n","\n","    cols = ['NamePunctCount', 'NameDigitCount', 'DescriptionDigitCount',\\\n","            'NameUpperCount', 'DescriptionPunctCount', 'DescriptionPunctCountRatio', \\\n","            'DescTokenCount', 'DescriptionUpperCount', 'NonAlphaDescCount', \\\n","            'NonAlphaNameCount', 'NameTokenCount', 'NameLowerCount', \\\n","            'NameDescTokenRatio', 'DescriptionLowerCount', 'NamePunctCountRatio',\\\n","            'MeanPrice_sc1','MeanPrice_sc2','MeanPrice_sc3','MeanPrice_brand_name']\n","\n","    df_num = scipy.sparse.csc_matrix(df[cols].values)\n","    print(df_num.shape)\n","\n","    df.drop(columns=cols,inplace=True)\n","\n","    print(f'[{round((time.time() - clock),2)}]Transform numeric  data completed.')\n","\n","    with open('pickle/count_vect','rb') as f:\n","          vect = pickle.load(f)\n","\n","    df_name = scipy.sparse.csr_matrix(vect.transform(df.name))\n","\n","    print(f'[{round((time.time() - clock),2)}]Transform name data completed.')\n","\n","\n","\n","    with open('pickle/tfidf_vect','rb') as f:\n","          vect = pickle.load(f)\n","\n","    df_text = scipy.sparse.csr_matrix(vect.transform(df.text))\n","\n","\n","    print(f'[{round((time.time() - clock),2)}]Transform text data completed.')\n","\n","\n","\n","\n","    # frequency encoding brands\n","    df_brand = high_categorical(df)\n","    df_brand= scipy.sparse.csr_matrix(df_brand)\n","\n","    print(f'[{round((time.time() - clock),2)}]Transform brand data completed.')\n","\n","\n","\n","    df_merge = scipy.sparse.hstack((df_brand,df_dummies, df_num,df_name, df_text ))\n","    print('Merge all data completed.')\n","\n","\n","    del df_brand,df_dummies,df_num,df_text,df_name\n","    print(f'[{round((time.time() - clock),2)}] {inspect.stack()[0][3]} complete')\n","\n","    return df_merge"]},{"cell_type":"code","execution_count":74,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":902},"executionInfo":{"elapsed":191322,"status":"ok","timestamp":1687707030737,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"},"user_tz":-330},"id":"NXF3JnR6wNrs"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-46-9e4814869026\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 3\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# df,y = Data(clock,n_rows =-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatapoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImpute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExtract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-5-9d6b04a7dd04\u003e\u001b[0m in \u001b[0;36mDatapoint\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 9\u001b[0;31m   \u001b[0mitem_condition_id\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Condition of your product ranging from 1 to 5 from good to worse \\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--\u003e 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}],"source":["clock = time.time()\n","# df,y = Data(clock,n_rows =-1)\n","df = Datapoint()\n","df = Impute(df,clock)\n","df = Extract_features(df,clock)\n","df = Make_text_column(df,clock)\n","df = Convert_to_predictor(df,clock)"]},{"cell_type":"code","execution_count":75,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1687707030737,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"},"user_tz":-330},"id":"6Er1oNJZHjzt","outputId":"287d8060-2199-4082-91e0-05b4c9ae799e"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'1.0.2'"]},"execution_count":75,"metadata":{},"output_type":"execute_result"}],"source":["sklearn.__version__"]},{"cell_type":"code","execution_count":76,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1687707030737,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"},"user_tz":-330},"id":"qsBL2THru8xi"},"outputs":[],"source":["def error(y_test, predictions):\n","  return np.sqrt(mean_squared_error( y_test, predictions ))"]},{"cell_type":"code","execution_count":77,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1687707030738,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"},"user_tz":-330},"id":"uEVN24LS_MR1","outputId":"e2288ff7-c901-4a70-d55f-1eebf331a714"},"outputs":[{"data":{"text/plain":["(1, 151022)"]},"execution_count":77,"metadata":{},"output_type":"execute_result"}],"source":["df.shape"]},{"cell_type":"code","execution_count":78,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1687707030738,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"},"user_tz":-330},"id":"p1iyRyGx_Y6K","outputId":"6eaacb31-c9e9-43d7-c03a-d96ea1f9ce0d"},"outputs":[{"data":{"text/plain":["0"]},"execution_count":78,"metadata":{},"output_type":"execute_result"}],"source":["np.isnan(df.data).sum()"]},{"cell_type":"code","execution_count":79,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1687707030738,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"},"user_tz":-330},"id":"LRQlqqNLO_Kj","outputId":"a9dd0ee3-f0ab-43c2-89d8-fd912f4410b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/mercari_capstone2\n"," 1-EDA.ipynb\t\t\t\t pickle\n","'2-featureization and pickleing.ipynb'\t price_log.pickle\n"," 3_Hyperparameter_tuning.ipynb\t\t tfidf.pickle\n","'4-train deploy.ipynb'\t\t\t train.tsv\n"," Mercari_to1.db\n"]}],"source":["!pwd\n","!ls"]},{"cell_type":"code","execution_count":82,"metadata":{"executionInfo":{"elapsed":637,"status":"ok","timestamp":1687707043431,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"},"user_tz":-330},"id":"t6hvmEKceKZ2"},"outputs":[],"source":["with open('pickle/models/ridge_weights','rb') as f:\n","  rdg = pickle.load(f)\n","op_rdg = rdg.predict(df.tocsr())"]},{"cell_type":"code","execution_count":83,"metadata":{"executionInfo":{"elapsed":1208,"status":"ok","timestamp":1687707045317,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"},"user_tz":-330},"id":"3R7g5RXeeTcV"},"outputs":[],"source":["with open('pickle/models/lbgm_coeffs','rb') as f:\n","  lgbm = pickle.load(f)\n","op_lgbm = lgbm.predict(df.tocsr())"]},{"cell_type":"code","execution_count":91,"metadata":{"executionInfo":{"elapsed":869,"status":"ok","timestamp":1687709008965,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"},"user_tz":-330},"id":"RlruDJ0Na4X4"},"outputs":[],"source":["le = 0.4395201992375062\n","re = 0.4443864614855584\n","tot = le+re"]},{"cell_type":"code","execution_count":92,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1687709009713,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"},"user_tz":-330},"id":"oHiABzWGbCn7"},"outputs":[],"source":["new_price = op_lgbm*re/tot + op_rdg*le/tot"]},{"cell_type":"code","execution_count":93,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1687709010247,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"},"user_tz":-330},"id":"hujG-mavqAUz"},"outputs":[],"source":["pred = {'ridge_price':op_rdg, 'lgbm_price':op_lgbm, 'weighted raito price' :new_price}"]},{"cell_type":"code","execution_count":94,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1687709010248,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"},"user_tz":-330},"id":"MgArN4PlrrxK","outputId":"5e9c4e03-9f48-462d-c50b-cd3f96cc7316"},"outputs":[{"data":{"text/plain":["{'ridge_price': array([57.6634912]),\n"," 'lgbm_price': array([3.49072175]),\n"," 'weighted raito price': array([30.57710647])}"]},"execution_count":94,"metadata":{},"output_type":"execute_result"}],"source":["pred"]}],"metadata":{"colab":{"machine_shape":"hm","name":"","provenance":[{"file_id":"1IzPbxNFQmmSYk9s14L4YjBfUgACn9mW2","timestamp":1609239498271},{"file_id":"1G7NKeneJNyRtcRxLVbbF9jYtRyuTOa-R","timestamp":1592749700622},{"file_id":"https://github.com/satyajitghana/TSAI-DeepVision-EVA4.0/blob/master/Utils/Colab_25GBRAM_GPU.ipynb","timestamp":1592043804148}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"26307a08bf6d44f092bda432b371ccba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3453f942d1184bfd8c7d7479aadef6bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"49f4adbd4fad47f5b9af8192765781fb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3f5619d29d24b6685f1baff85eb2d56","placeholder":"​","style":"IPY_MODEL_26307a08bf6d44f092bda432b371ccba","value":" 1/1 [00:00\u0026lt;00:00, 54.40it/s]"}},"4f7be424a774491b8789a0440346407f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5de61cdd3c8f459081db768ff087b54a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_82859e087ab14bc3a454aab632741820","IPY_MODEL_9ac4198a83404bbc8031a1ab2fa59cce","IPY_MODEL_49f4adbd4fad47f5b9af8192765781fb"],"layout":"IPY_MODEL_b52e0e80d3c04986a9a19c92cc750583"}},"82859e087ab14bc3a454aab632741820":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e912a932aa4a4b548a2f0bbd1b7de4ab","placeholder":"​","style":"IPY_MODEL_4f7be424a774491b8789a0440346407f","value":"100%"}},"9ac4198a83404bbc8031a1ab2fa59cce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f52fe20365564810b483e513a3c1201d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3453f942d1184bfd8c7d7479aadef6bb","value":1}},"b52e0e80d3c04986a9a19c92cc750583":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3f5619d29d24b6685f1baff85eb2d56":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e912a932aa4a4b548a2f0bbd1b7de4ab":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f52fe20365564810b483e513a3c1201d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}