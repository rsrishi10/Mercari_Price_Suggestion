{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1IzPbxNFQmmSYk9s14L4YjBfUgACn9mW2","timestamp":1609239498271},{"file_id":"1G7NKeneJNyRtcRxLVbbF9jYtRyuTOa-R","timestamp":1592749700622},{"file_id":"https://github.com/satyajitghana/TSAI-DeepVision-EVA4.0/blob/master/Utils/Colab_25GBRAM_GPU.ipynb","timestamp":1592043804148}],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"21320ad318ad4370940c0d5fb5d27811":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_744cfd6322404b0da02673dbf4ca15bb","IPY_MODEL_b17e6ec08e6a4817b4117d559586c63e","IPY_MODEL_7d6008f4b91e4239b0dfb4896132be95"],"layout":"IPY_MODEL_77034ef40d1447f1a99cabc4ebbc47d8"}},"744cfd6322404b0da02673dbf4ca15bb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_04f653e454e84b21b0d7c6a617ee4956","placeholder":"​","style":"IPY_MODEL_d09beb02fdfb4439a7d7b9ffacf5353c","value":"100%"}},"b17e6ec08e6a4817b4117d559586c63e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_db9dce82f68a4266b80ced92ddbd1d2d","max":1481661,"min":0,"orientation":"horizontal","style":"IPY_MODEL_193649884f6840c99c0e6e3949b1c608","value":1481661}},"7d6008f4b91e4239b0dfb4896132be95":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a9ae2e1e3204c38b69ad09a73806c9c","placeholder":"​","style":"IPY_MODEL_7b0e9169a391424b9a6dc850b4524159","value":" 1481661/1481661 [00:36&lt;00:00, 27323.17it/s]"}},"77034ef40d1447f1a99cabc4ebbc47d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04f653e454e84b21b0d7c6a617ee4956":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d09beb02fdfb4439a7d7b9ffacf5353c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db9dce82f68a4266b80ced92ddbd1d2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"193649884f6840c99c0e6e3949b1c608":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1a9ae2e1e3204c38b69ad09a73806c9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b0e9169a391424b9a6dc850b4524159":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mPKnmrJgqq32","executionInfo":{"status":"ok","timestamp":1687691445722,"user_tz":-330,"elapsed":16318,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"}},"outputId":"8c9483fe-005d-47f8-e0ae-1bb410f49a3b"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0QJOq9G5qrCx","executionInfo":{"status":"ok","timestamp":1687691445722,"user_tz":-330,"elapsed":4,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"}},"outputId":"2cbe9b97-cc87-4f1d-9c8c-f30c6dbb6665"},"source":["import os\n","os.chdir(\"/content/drive/My Drive/mercari_capstone2\")\n","!ls -l"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["total 344117\n","-rw------- 1 root root   2441360 Jun 23 10:05  1-EDA.ipynb\n","-rw------- 1 root root     40878 Jun 25 11:05 '2-featureization and pickleing.ipynb'\n","-rw------- 1 root root     51452 Jun 25 11:10  3_Hyperparameter_tuning.ipynb\n","-rw------- 1 root root     49476 Jun 25 11:07 '4-train deploy.ipynb'\n","-rw------- 1 root root    122880 Jun 24 19:02  Mercari_to1.db\n","drwx------ 2 root root      4096 Jun 23 11:42  pickle\n","-rw------- 1 root root  11853920 Jun 23 12:33  price_log.pickle\n","-rw------- 1 root root         0 Jun 25 11:05  tfidf.pickle\n","-rw------- 1 root root 337809843 Nov 11  2017  train.tsv\n"]}]},{"cell_type":"code","source":["!pip uninstall scikit-learn -y\n","!pip install scikit-learn==1.0.2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qC9LA7jaJLrW","executionInfo":{"status":"ok","timestamp":1687691456004,"user_tz":-330,"elapsed":10284,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"}},"outputId":"999bfcc3-eb55-4c94-fb99-7a5b32b1a02d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: scikit-learn 1.2.2\n","Uninstalling scikit-learn-1.2.2:\n","  Successfully uninstalled scikit-learn-1.2.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scikit-learn==1.0.2\n","  Downloading scikit_learn-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.22.4)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.10.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (3.1.0)\n","Installing collected packages: scikit-learn\n","Successfully installed scikit-learn-1.0.2\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"faDikRZQqrDj","executionInfo":{"status":"ok","timestamp":1687691459147,"user_tz":-330,"elapsed":3146,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"}},"outputId":"30410d10-86f7-4d6c-e994-7199dd209210"},"source":["#importing modules/libraries\n","import pandas as pd\n","import numpy as np\n","import scipy\n","import gc\n","\n","from tqdm.notebook import tqdm\n","import re\n","import random\n","import pickle\n","import inspect\n","import time\n","\n","import sklearn\n","from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, LabelBinarizer,OneHotEncoder\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import mean_squared_error\n","import lightgbm as lgb\n","from sklearn.linear_model import Ridge\n","\n","\n","import string\n","\n","import nltk\n","nltk.download(\"stopwords\")\n","\n","nltk.download('wordnet')\n","\n","from nltk.corpus import stopwords\n","from nltk.stem.wordnet import WordNetLemmatizer\n","\n","\n","\n","\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]}]},{"cell_type":"code","metadata":{"id":"2W3LxvsTqrFt","executionInfo":{"status":"ok","timestamp":1687691459148,"user_tz":-330,"elapsed":3,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"}}},"source":["#function to load data\n","def Data(clock,n_rows):\n","  if int(n_rows) == -1:\n","    df = pd.read_csv('train.tsv', sep='\\t')\n","  else:\n","    df = pd.read_csv('train.tsv', sep='\\t',nrows =n_rows)\n","\n","  df = df.drop(['train_id'], axis =1)\n","  df = df.drop(df[df.price <= 1.0].index)\n","  df.reset_index(inplace=True)\n","  df['item_condition_id'] = df['item_condition_id'].astype('category')\n","  # df =df[df['brand_name'].notnull()]\n","  y = np.log1p(df.price)\n","  gc.collect()\n","  print(f'[{round((time.time() - clock),2)}] {inspect.stack()[0][3]} completed')\n","  return df, y,round(df.shape[0]*0.8),df.shape[0]"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"MtTie9hYqrG3","executionInfo":{"status":"ok","timestamp":1687691459148,"user_tz":-330,"elapsed":3,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"}}},"source":["def Impute(df, tr_len,clock):\n","  # imputing with a 'abs' or absent\n","    df['name'].fillna(value='abs', inplace=True)\n","    df['item_description'].fillna(value='No Description Yet', inplace=True)\n","\n","  # using brands from train data only\n","    tr = df.iloc[:tr_len,:]\n","    test = df.iloc[tr_len:,:]\n","\n","  # imputing with a 'abs' or absent for train data and using  brand\n","  # names form train data only as target and freqency encoding done\n","  # in later sections ofnotebook so avoiding data leakage\n","    tr['brand_name'].fillna(value='abs',inplace=True)\n","    brand_name =tr.brand_name.unique()\n","    # with open('pickle/impute_brand_name','wb') as f:\n","    #   pickle.dump(brand_name,f)\n","    test.loc[~test['brand_name'].isin(brand_name),'brand_name'] = 'abs'\n","\n","    tr['category_name'].fillna(value='abs',inplace=True)\n","    category_name =tr.category_name.unique()\n","    # with open('pickle/impute_category_name','wb') as f:\n","    #   pickle.dump(category_name,f)\n","    test.loc[~test['category_name'].isin(category_name),'category_name'] = 'abs'\n","\n","   # print fucntion completion time and with function name\n","    print(f'[{round((time.time() - clock),2)}] {inspect.stack()[0][3]} completed')\n","    del brand_name\n","    del df\n","    gc.collect()\n","    return pd.concat([tr,test],axis=0, ignore_index=True)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pi3LCsb_qrIE","executionInfo":{"status":"ok","timestamp":1687691459148,"user_tz":-330,"elapsed":3,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"}}},"source":["# a category column contains Nan or 3 or more sub category in it upto 5\n","# as rows with more than 3 or less than 3 categories are less than 0.1 percent,\n","# we make only 3 new cols with segregated category names\n","def sub_cat(row):\n","      try:\n","        split = row.split('/')\n","        if len(split) >= 3:\n","          return split[0],split[1],split[2]\n","        if len(split) == 2:\n","          return split[0], split[1], 'abs'\n","        elif len(split) == 1:\n","          return split[0], 'abs', 'abs'\n","        else:\n","          return 'abs', 'abs', 'abs'\n","      except Exception:\n","          return  'abs', 'abs', 'abs'\n","\n","# extracting extra features from data\n","def Extract_features(df,tr_len,clock):\n","  # regex used later in this section to count number of them in a text column\n","    RE_PUNCTUATION = '|'.join([re.escape(x) for x in string.punctuation])\n","    non_alphanumpunct = re.compile(u'[^A-Za-z0-9\\.?!,; \\(\\)\\[\\]\\'\\\"\\$]+')\n","\n","  # extracting sub categories\n","    print(f'[{round(time.time()-clock)}]Extracting Subcat')\n","    df['sc1'], df['sc2'],df['sc3'] =  zip(*df['category_name'].apply(sub_cat))\n","    df.drop(columns='category_name',inplace = True)\n","    df['sc1'] = df['sc1'].astype('category')\n","    df['sc2'] = df['sc2'].astype('category')\n","    df['sc3'] = df['sc3'].astype('category')\n","\n","  # has description or not/ missing value added as a feature\n","    print(f'[{round(time.time()-clock)}]Extracting HasDescription ')\n","    df['HasDescription'] = 1\n","    df.loc[df['item_description']=='No description yet', 'HasDescription'] = 0\n","    df['HasDescription'] =df['HasDescription'].astype('category')\n","\n","  # has price or not/ [rm] values in textual columns are indicative of presence\n","  # price in the data which has been cleaned as suggested by the compition itself\n","    print(f'[{round(time.time()-clock)}]Extracting HasPrice ')\n","    df['HasPrice'] = 0\n","    df.loc[df['item_description'].str.contains('[rm]', regex=False), 'HasPrice'] = 1\n","    df.loc[df['name'].str.contains('[rm]', regex=False), 'HasPrice'] = 1\n","    df['HasPrice'] =df['HasPrice'].astype('category')\n","\n","    gc.collect()\n","  # counting number of tokens in textual columns\n","    print(f'[{round(time.time()-clock)}]Extracting Token Count ')\n","    df['NameTokenCount'] = df['name'].str.split().apply(len)\n","    df['DescTokenCount'] = df['item_description'].str.split().apply(len)\n","    df['NameTokenCount'] = df['NameTokenCount'].astype('uint32')\n","    df['DescTokenCount'] = df['DescTokenCount'].astype('uint32')\n","\n","  # ratio of token token counts in name and description columns(2 textual cols)\n","\n","    print(f'[{round(time.time()-clock)}]Extracting Name to Desc token Ratio ')\n","    df['NameDescTokenRatio'] = df['NameTokenCount']/df['DescTokenCount']\n","    df['NameDescTokenRatio'] =df['NameDescTokenRatio'].astype('float32')\n","\n","  # adding missing value as a feature for brand\n","    print(f'[{round(time.time()-clock)}]Extracting HasBrand ')\n","    df['HasBrand'] =1\n","    df.loc[df['brand_name'] == 'abs', 'HasBrand'] = 0\n","    df['HasBrand'] =df['HasBrand'].astype('category')\n","\n","  # counting uppper and lower count of characters as EDA suggested phoney/\n","  # counterfiet items when listed uses too many bold and Caps charactes with emojis\n","    print(f'[{round(time.time()-clock)}]Extracting Lower count ')\n","    df['NameLowerCount'] = df.name.str.count('[a-z]')\n","    df['DescriptionLowerCount'] = df.item_description.str.count('[a-z]')\n","    df['NameLowerCount'] =df['NameLowerCount'].astype('uint32')\n","    df['DescriptionLowerCount'] =df['DescriptionLowerCount'].astype('uint32')\n","\n","\n","    print(f'[{round(time.time()-clock)}]Extracting Upper count ')\n","    df['NameUpperCount'] = df.name.str.count('[A-Z]')\n","    df['DescriptionUpperCount'] = df.item_description.str.count('[A-Z]')\n","    df['NameUpperCount'] =df['NameUpperCount'].astype('uint32')\n","    df['DescriptionUpperCount'] =df['DescriptionUpperCount'].astype('uint32')\n","\n","  # punctuation count\n","    print(f'[{round(time.time()-clock)}]Extracting Punctuation Count ')\n","    df['NamePunctCount'] = df.name.str.count(RE_PUNCTUATION)\n","    df['DescriptionPunctCount'] = df.item_description.str.count(RE_PUNCTUATION)\n","    df['NamePunctCount'] =df['NamePunctCount'].astype('uint32')\n","    df['DescriptionPunctCount'] =df['DescriptionPunctCount'].astype('uint32')\n","\n","  # punct count ratio\n","    print(f'[{round(time.time()-clock)}]Extracting Punctuation Ratio ')\n","    df['NamePunctCountRatio'] = df['NamePunctCount'] / df['NameTokenCount']\n","    df['DescriptionPunctCountRatio'] = df['DescriptionPunctCount'] / df['DescTokenCount']\n","    df['NamePunctCountRatio'] =df['NamePunctCountRatio'].astype('float32')\n","    df['DescriptionPunctCountRatio'] =df['DescriptionPunctCountRatio'].astype('float32')\n","\n","  # digit count( if model can get a sense of bundled items)\n","    print(f'[{round(time.time()-clock)}]Extracting Digit count ')\n","    df['NameDigitCount'] = df.name.str.count('[0-9]')\n","    df['DescriptionDigitCount'] = df.item_description.str.count('[0-9]')\n","    df['NameDigitCount'] =df['NameDigitCount'].astype('uint32')\n","    df['DescriptionDigitCount'] =df['DescriptionDigitCount'].astype('uint32')\n","\n","  # emoji and/or other nonalphanum count\n","    print(f'[{round(time.time()-clock)}]Extracting NonAlphaNum count ')\n","    df['NonAlphaDescCount'] = df['item_description'].str.count(non_alphanumpunct)\n","    df['NonAlphaNameCount'] = df['name'].str.count(non_alphanumpunct)\n","    df['NonAlphaDescCount'] =df['NonAlphaDescCount'].astype('uint32')\n","    df['NonAlphaNameCount'] =df['NonAlphaNameCount'].astype('uint32')\n","\n","\n","    cols = set(df.columns.values)\n","    non_num_col = {'name', 'item_condition_id', 'brand_name',\n","                  'shipping', 'item_description', 'sc1',\n","                  'sc2', 'sc3','HasDescription','HasPrice','HasBrand',\n","                   'price','index'\n","                  }\n","\n","    cols_to_normalize = cols - non_num_col\n","\n","  # normalizing all the counts and ratios\n","    print(f'[{round(time.time()-clock)}]Normalizing')\n","    df_to_normalize = df[list(cols_to_normalize)]\n","    df_to_normalize = (df_to_normalize - df_to_normalize.min()) / (df_to_normalize.max() - df_to_normalize.min())\n","\n","    # with open('pickle/normalize_min','wb') as f:\n","    #   pickle.dump(df_to_normalize.min(),f)\n","    # with open('pickle/normalize_max','wb') as f:\n","    #   pickle.dump(df_to_normalize.max(),f)\n","\n","    df = df[list(non_num_col)]\n","    df = pd.concat([df, df_to_normalize],axis=1)\n","\n","    df.drop(columns='index',inplace=True)\n","\n","    del(df_to_normalize)\n","    gc.collect()\n","\n","\n","    '''  extracting mean categorical and brand price with minding data leakage with addition of\n","    random noise so making data more robust. An idea taken form a some youtube video of a\n","    kaggle grandmaster . This noise addition clearly has impacted the performace of model very positively. '''\n","\n","    print(f'[{round(time.time()-clock)}]Extracting Mean price Categories')\n","\n","    tr = df.iloc[:tr_len,:]\n","    ts = df.iloc[tr_len:,:]\n","    lst = ['sc1','sc2','sc3', 'brand_name']\n","\n","  #imputing values for nan with mean\n","    def boundary_case(hmap,key):\n","      try:\n","        return float(hmap[key])*np.random.normal(1,0.1)\n","      except:\n","        '''  when cases in test data are not\n","        present in train data mean_dict[feat] returns a nan to tackle that this part\n","        has been added( tho with normal usage it does not occur as this has been\n","        taken care of in  the imputation part itself , i had an experiment run which\n","        produced those cases so made this part as permanent only) '''\n","        np.mean(list(hmap.values()))*np.random.normal(1,0.1)\n","\n","\n","    for feat in lst:\n","        ''' for every categorical column in the list above  finding the mean price of\n","        every category in it and adding that price in a column with a noise added to it\n","        *np.randon.normal(1,0.1)'''\n","        mean_dc = (tr.groupby(feat)['price'].mean()/np.max(df.groupby(feat)['price'].mean())).to_dict()\n","        # with open(f'pickle/mean_dc_{feat}','wb') as f:\n","        #   pickle.dump(mean_dc,f)\n","        tr['MeanPrice_'+feat] = tr[feat].apply(lambda x : boundary_case(mean_dc,x)).astype(np.float32)\n","        tr['MeanPrice_'+feat].fillna( np.mean(list(mean_dc.values()))*np.random.normal(1,0.1), inplace=True  )\n","\n","\n","        ts['MeanPrice_'+feat] = ts[feat].apply(lambda x : boundary_case(mean_dc,x)).astype(np.float32)\n","        ts['MeanPrice_'+feat].fillna( np.mean(list(mean_dc.values()))*np.random.normal(1,0.1), inplace=True  )\n","\n","\n","    tr.drop(columns='price',inplace = True)\n","    ts.drop(columns='price',inplace = True)\n","\n","    print(f'[{round((time.time() - clock),2)}] {inspect.stack()[0][3]} completed')\n","\n","    del df,mean_dc\n","    gc.collect()\n","    return pd.concat([tr,ts],axis=0)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"ISKXX4lOqrKK","executionInfo":{"status":"ok","timestamp":1687691459812,"user_tz":-330,"elapsed":3,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"}}},"source":["def Make_text_column(df,clock):\n","    '''As we saw in EDA that brands with NAN values can be imputed with names and item_desciption\n","    columns as there are brand names prevelent with more than 40 to 45 percent of chance.\n","    So instead of imputing so many brands (43 percent), just creating a new column by merging\n","    brands_name, name and item_description and making a text column and letting tfidf taking care of it.\n","    '''\n","\n","    df['text'] = df['name'].astype(str)+' '+df['brand_name'].str.strip().astype(str)+' '+df['item_description'].str.strip().astype(str)\n","    df = df.drop(columns=['item_description'])\n","\n","\n","    def decontracted(text):\n","    # tried many kinds of regex to clean the data but final result wasnt effected much wiht\n","    # this part so only doing necessary onces\n","        try:\n","            text = re.sub(u\"won't\", \"will not\", text)\n","            text = re.sub(u\"can\\'t\", \"can not\", text)\n","            text = re.sub(u\"n\\'t\", \" not\", text)\n","            text = re.sub(u\"\\'t\", \" not\", text)\n","            # separating digits for a sense of count if bundled items sold\n","            text = u\" \".join(re.split('(\\d+)',text) )\n","\n","        except:\n","            print('error')\n","        return text\n","\n","    def clean(df,col):\n","        non_alphanums = re.compile(u'[^A-Za-z0-9 ]+')\n","        # wl = WordNetLemmatizer()\n","\n","        preprocessed_text = []\n","        for _,sentance in tqdm(df[col].iteritems(),total=df.shape[0]):\n","\n","            sentance = decontracted(sentance)\n","\n","            # nonalphanumeric character removal\n","            sentance = non_alphanums.sub(u' ', sentance)\n","\n","            ''' did not lemmatize cause takes a lot of time and has negligible to no\n","            effect on performance\n","            did not convert to lower case as this takes a lot of time and can be done\n","            interensicly within TFIDF and Count vectorization along with text standardization'''\n","            # lemmetizing\n","            # sentance = ' '.join(wl.lemmatize(word.strip()) for word in sentance.split())\n","            sentance = ' '.join(word.strip() for word in sentance.split())\n","\n","            preprocessed_text.append(sentance)\n","        df[col] = pd.Series(preprocessed_text).values\n","        del preprocessed_text\n","        return df\n","\n","\n","    print('Cleaning text')\n","    df= clean(df,'text')\n","    print(f'Done')\n","\n","    print(f'[{round((time.time() - clock),2)}] {inspect.stack()[0][3]} completed')\n","\n","\n","    gc.collect()\n","    return df"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"_jckn-IjqrN1","executionInfo":{"status":"ok","timestamp":1687691459812,"user_tz":-330,"elapsed":3,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"}}},"source":["\n","def high_categorical(df,ts,col='brand_name'):\n","\n","        ''' As brand has close to 5000 categories, converting it to numbers\n","        by Frequency encoding brand with minding data leakage, Tho  lgbm handles high order\n","        categorical columns efficiently this encoding boosted performance instead of just\n","        using OHE, also it is with addition of random noise so making data more robust and\n","        enhancing performance I had chosen to do the same with sub categorical columns but the\n","        did not perform that well and took extra memory space too'''\n","\n","        dictionary_frq = df[col].value_counts().to_dict()\n","        dict_replace = {k:(v/max(dictionary_frq.values()) * np.random.normal(1,0.01)) for k,v in dictionary_frq.items()}\n","        # with open('pickle/brand_freq_dict','wb') as f:\n","        #   pickle.dump(dict_replace,f)\n","        col_cat: pd.Series.astype('float16') = df[col].map(dict_replace)\n","        col_cat_ts: pd.Series.astype('float16') = ts[col].map(dict_replace)\n","        del dictionary_frq\n","        del dict_replace\n","        gc.collect()\n","        return col_cat.values.reshape(-1,1),col_cat_ts.values.reshape(-1,1)\n","\n","'''Converting all the data till yet to numeric form if not yet done'''\n","def Convert_to_predictor(df, tr_len,clock,stopwords=stopwords,high_categorical=high_categorical):\n","    try:\n","      df.drop(columns='index',inplace = True)\n","    except:\n","      pass\n","\n","    print(f'[{round((time.time() - clock),2)}]Transform data start.')\n","\n","    cv = OneHotEncoder()\n","    item_condition_id = cv.fit_transform(df['item_condition_id'].astype(int).iloc[:].values.reshape(-1,1))\n","    # with open('pickle/item_condition_id','wb') as f:\n","    #   pickle.dump(cv.fit(df['item_condition_id'].astype(int).iloc[:].values.reshape(-1,1)),f)\n","    print(f'[{round((time.time() - clock),2)}]Transform item_condition_id  data completed.')\n","\n","\n","    cv = OneHotEncoder()\n","    shipping = cv.fit_transform(df['shipping'].astype(int).iloc[:].values.reshape(-1,1))\n","    # with open('pickle/shipping','wb') as f:\n","    #   pickle.dump(cv.fit(df['shipping'].astype(int).iloc[:].values.reshape(-1,1)),f)\n","    print(f'[{round((time.time() - clock),2)}]Transform shipping  data completed.')\n","\n","\n","    cv = OneHotEncoder()\n","    HasDescription = cv.fit_transform(df['HasDescription'].astype(int).iloc[:].values.reshape(-1,1))\n","    # with open('pickle/HasDescription','wb') as f:\n","    #   pickle.dump(cv.fit(df['HasDescription'].astype(int).iloc[:].values.reshape(-1,1)),f)\n","    print(f'[{round((time.time() - clock),2)}]Transform HasDescription  data completed.')\n","\n","\n","    cv = OneHotEncoder()\n","    HasPrice = cv.fit_transform(df['HasPrice'].astype(int).iloc[:].values.reshape(-1,1))\n","    # with open('pickle/HasPrice','wb') as f:\n","    #   pickle.dump(cv.fit(df['HasPrice'].astype(int).iloc[:].values.reshape(-1,1)),f)\n","    print(f'[{round((time.time() - clock),2)}]Transform HasPrice  data completed.')\n","\n","\n","    cv = OneHotEncoder()\n","    HasBrand = cv.fit_transform(df['HasBrand'].astype(int).iloc[:].values.reshape(-1,1))\n","    # with open('pickle/HasBrand','wb') as f:\n","    #   pickle.dump(cv.fit(df['HasBrand'].astype(int).iloc[:].values.reshape(-1,1)),f)\n","    print(f'[{round((time.time() - clock),2)}]Transform HasBrand  data completed.')\n","\n","    cv = OneHotEncoder()\n","    sc1 = cv.fit_transform(df.sc1.astype(str).iloc[:].values.reshape(-1,1))\n","    # with open('pickle/sc1','wb') as f:\n","    #   pickle.dump(cv.fit(df.sc1.astype(str).iloc[:].values.reshape(-1,1)),f)\n","    print(f'[{round((time.time() - clock),2)}]Transform sc1  data completed.')\n","\n","    cv = OneHotEncoder()\n","    sc2 = cv.fit_transform(df.sc2.astype(str).iloc[:].values.reshape(-1,1))\n","    # with open('pickle/sc2','wb') as f:\n","    #   pickle.dump(cv.fit(df.sc2.astype(str).iloc[:].values.reshape(-1,1)),f)\n","    print(f'[{round((time.time() - clock),2)}]Transform sc2  data completed.')\n","\n","    cv = OneHotEncoder()\n","    sc3 = cv.fit_transform(df.sc3.astype(str).iloc[:].values.reshape(-1,1))\n","    # with open('pickle/sc3','wb') as f:\n","    #   pickle.dump(cv.fit(df.sc3.astype(str).iloc[:].values.reshape(-1,1)),f)\n","    print(f'[{round((time.time() - clock),2)}]Transform sc3  data completed.')\n","\n","\n","\n","\n","    df_dummies = scipy.sparse.hstack([item_condition_id, shipping, HasDescription, HasPrice,HasBrand,sc1,sc2,sc3])\n","    print(df_dummies.shape)\n","\n","    df.drop(columns=['item_condition_id', 'shipping','HasDescription', 'HasPrice','HasBrand'],inplace=True)\n","    df.drop(columns=['sc1','sc2','sc3'],inplace=True)\n","\n","\n","\n","    print(f'[{round((time.time() - clock),2)}]Transform categories data completed.')\n","\n","\n","    cols = ['NamePunctCount', 'NameDigitCount', 'DescriptionDigitCount',\\\n","            'NameUpperCount', 'DescriptionPunctCount', 'DescriptionPunctCountRatio', \\\n","            'DescTokenCount', 'DescriptionUpperCount', 'NonAlphaDescCount', \\\n","            'NonAlphaNameCount', 'NameTokenCount', 'NameLowerCount', \\\n","            'NameDescTokenRatio', 'DescriptionLowerCount', 'NamePunctCountRatio',\\\n","            'MeanPrice_sc1','MeanPrice_sc2','MeanPrice_sc3','MeanPrice_brand_name']\n","\n","    df_num = scipy.sparse.csc_matrix(df[cols].values)\n","\n","    df.drop(columns=cols,inplace=True)\n","    print(df_num.shape)\n","\n","    print(f'[{round((time.time() - clock),2)}]Transform numeric  data completed.')\n","\n","    gc.collect()\n","\n","    tr = df.iloc[:tr_len,:]\n","    test = df.iloc[tr_len:,:]\n","    gc.collect()\n","    del df\n","    vect = CountVectorizer(ngram_range=(1,3),min_df=5, max_df=0.85,\n","                         lowercase=True, max_features=50000,\n","                        analyzer='word', strip_accents = 'ascii',\n","                        stop_words= \"english\")\n","\n","    tr_name = scipy.sparse.csr_matrix(vect.fit_transform(tr.name))\n","    ts_name = scipy.sparse.csr_matrix(vect.transform(test.name))\n","    df_name = scipy.sparse.vstack((tr_name,ts_name),format='csc')\n","    # with open('pickle/count_vect','wb') as f:\n","    #       pickle.dump(vect.fit(tr.name),f)\n","    tr.drop(columns=['name'],inplace=True)\n","    test.drop(columns=['name'],inplace=True)\n","\n","    del vect,ts_name,tr_name\n","    print(df_name.shape)\n","\n","    print(f'[{round((time.time() - clock),2)}]Transform name data completed.')\n","\n","\n","\n","    vect = TfidfVectorizer(ngram_range=(1,3),min_df=5, max_df=0.85,\n","                         lowercase=True, max_features=100000,\n","                        analyzer='word', strip_accents = 'ascii', smooth_idf=True,stop_words= \"english\")\n","\n","    tr_text = scipy.sparse.csr_matrix(vect.fit_transform(tr.text))\n","    ts_text = scipy.sparse.csr_matrix(vect.transform(test.text))\n","    df_text = scipy.sparse.vstack((tr_text,ts_text),format='csc')\n","    # with open('pickle/tfidf_vect','wb') as f:\n","    #       pickle.dump(vect.fit(tr.text),f)\n","    tr.drop(columns=['text'],inplace=True)\n","    test.drop(columns=['text'],inplace=True)\n","\n","    del vect,ts_text,tr_text\n","    print(df_text.shape)\n","\n","    print(f'[{round((time.time() - clock),2)}]Transform text data completed.')\n","\n","\n","\n","\n","    # frequency encoding brands\n","    tr_brand,ts_brand = high_categorical(tr,test)\n","    tr_brand,ts_brand = scipy.sparse.csr_matrix(tr_brand),scipy.sparse.csr_matrix(ts_brand)\n","    df_brand = scipy.sparse.vstack((tr_brand,ts_brand),format='csc')\n","    print(df_brand.shape)\n","\n","    tr.drop(columns=['brand_name'],inplace=True)\n","    test.drop(columns=['brand_name'],inplace=True)\n","    del tr_brand,ts_brand,high_categorical\n","\n","    print(f'[{round((time.time() - clock),2)}]Transform brand data completed.')\n","\n","\n","\n","    df_merge = scipy.sparse.hstack((df_brand,df_dummies, df_num,df_name, df_text ))\n","    print('Merge all data completed.')\n","\n","\n","    del df_brand,df_num,df_text,df_name,df_dummies\n","    print(f'[{round((time.time() - clock),2)}] {inspect.stack()[0][3]} complete')\n","\n","    gc.collect()\n","    return df_merge"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":778,"referenced_widgets":["21320ad318ad4370940c0d5fb5d27811","744cfd6322404b0da02673dbf4ca15bb","b17e6ec08e6a4817b4117d559586c63e","7d6008f4b91e4239b0dfb4896132be95","77034ef40d1447f1a99cabc4ebbc47d8","04f653e454e84b21b0d7c6a617ee4956","d09beb02fdfb4439a7d7b9ffacf5353c","db9dce82f68a4266b80ced92ddbd1d2d","193649884f6840c99c0e6e3949b1c608","1a9ae2e1e3204c38b69ad09a73806c9c","7b0e9169a391424b9a6dc850b4524159"]},"id":"3VVMJKvwrVNz","executionInfo":{"status":"ok","timestamp":1687691868554,"user_tz":-330,"elapsed":408744,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"}},"outputId":"157538b1-e662-4f43-cbd6-87e86c6817d3"},"source":["clock =time.time()\n","df,y, tr_len,whole_tr= Data(clock,n_rows = -1)\n","gc.collect()\n","df = Impute(df,tr_len,clock)\n","gc.collect()\n","df = Extract_features(df,tr_len,clock)\n","gc.collect()\n","df = Make_text_column(df,clock)\n","gc.collect()\n","df = Convert_to_predictor(df,tr_len,clock)\n","gc.collect()"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[9.99] Data completed\n","[11.3] Impute completed\n","[12]Extracting Subcat\n","[18]Extracting HasDescription \n","[18]Extracting HasPrice \n","[20]Extracting Token Count \n","[38]Extracting Name to Desc token Ratio \n","[38]Extracting HasBrand \n","[38]Extracting Lower count \n","[71]Extracting Upper count \n","[79]Extracting Punctuation Count \n","[85]Extracting Punctuation Ratio \n","[85]Extracting Digit count \n","[90]Extracting NonAlphaNum count \n","[98]Normalizing\n","[99]Extracting Mean price Categories\n","[104.97] Extract_features completed\n","Cleaning text\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1481661 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21320ad318ad4370940c0d5fb5d27811"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Done\n","[143.9] Make_text_column completed\n","[144.22]Transform data start.\n","[144.45]Transform item_condition_id  data completed.\n","[144.56]Transform shipping  data completed.\n","[144.67]Transform HasDescription  data completed.\n","[144.77]Transform HasPrice  data completed.\n","[144.89]Transform HasBrand  data completed.\n","[145.76]Transform sc1  data completed.\n","[146.72]Transform sc2  data completed.\n","[147.89]Transform sc3  data completed.\n","(1481661, 1002)\n","[148.65]Transform categories data completed.\n","(1481661, 19)\n","[150.32]Transform numeric  data completed.\n","(1481661, 50000)\n","[185.7]Transform name data completed.\n","(1481661, 100000)\n","[406.39]Transform text data completed.\n","(1481661, 1)\n","[407.12]Transform brand data completed.\n","Merge all data completed.\n","[409.16] Convert_to_predictor complete\n"]},{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uEVN24LS_MR1","executionInfo":{"status":"ok","timestamp":1687691868554,"user_tz":-330,"elapsed":10,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"}},"outputId":"ec531764-8db4-49d0-8919-8d9f5243981d"},"source":["df.shape"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1481661, 151022)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"_i3pqFwYNnWr","executionInfo":{"status":"ok","timestamp":1687691873950,"user_tz":-330,"elapsed":5397,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"}}},"source":["with open('tfidf.pickle','wb') as f:\n","  pickle.dump(df,f)"],"execution_count":13,"outputs":[]},{"cell_type":"code","source":["with open('price_log.pickle','wb') as f:\n","  pickle.dump(y,f)"],"metadata":{"id":"1JLHhIvvFNJU","executionInfo":{"status":"ok","timestamp":1687691874558,"user_tz":-330,"elapsed":616,"user":{"displayName":"Rishabh singh","userId":"13746845429811791925"}}},"execution_count":14,"outputs":[]}]}